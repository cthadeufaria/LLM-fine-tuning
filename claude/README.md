# Legal BERT Seq2Seq - Portuguese Immigration Law Consultant

A specialized PEFT (Parameter-Efficient Fine-Tuning) pipeline for fine-tuning BERT as a sequence-to-sequence model to serve as a legal consultant for Portuguese immigration matters. Built with HuggingFace Transformers and PEFT/LoRA adapters.

## üèõÔ∏è Overview

This project transforms a Portuguese BERT model into an encoder-decoder architecture specifically designed for legal consultation in Portuguese immigration law. The model can answer questions about visas, residency permits, citizenship, family reunification, and other immigration-related topics.

## ‚ú® Key Features

- **Legal Domain Specialization**: Fine-tuned specifically for Portuguese immigration law
- **Portuguese Language**: Uses `neuralmind/bert-base-portuguese-cased` as base model
- **Seq2Seq Architecture**: BERT encoder with decoder for generative responses
- **PEFT Integration**: Memory-efficient training with LoRA adapters
- **Legal Token Vocabulary**: Special tokens for legal concepts ([LEGAL_Q], [LEGAL_A], [IMMIGRATION], etc.)
- **Interactive Consultation**: Chat-like interface for legal questions

## üöÄ Quick Start

### 1. Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Or run the setup script
./setup.sh
```

### 2. Create Training Data and Train

```bash
# Create sample legal data and start training
python main.py --data_path legal_data.json --create_sample_data --max_steps 100
```

### 3. Interactive Legal Consultation

```bash
# Start interactive legal consultation
python inference.py \
  --base_model neuralmind/bert-base-portuguese-cased \
  --peft_model ./legal_fine_tuned_model \
  --interactive
```

## üìã Data Format

The training data should follow this legal Q&A format:

```json
[
  {
    "question": "Quanto tempo demora para obter um visto de trabalho em Portugal?",
    "context": "Cidad√£o brasileiro interessado em trabalhar em Portugal", 
    "answer": "O prazo para obten√ß√£o de um visto de trabalho em Portugal varia entre 15 a 60 dias √∫teis..."
  }
]
```

## üéì Training Examples

### Basic Training
```bash
python main.py \
  --data_path legal_immigration_data.json \
  --output_dir ./legal_model \
  --num_epochs 5 \
  --learning_rate 5e-5 \
  --batch_size 4
```

### Advanced Training
```bash
python main.py \
  --model_name neuralmind/bert-base-portuguese-cased \
  --data_path comprehensive_legal_data.json \
  --output_dir ./advanced_legal_model \
  --max_length 1024 \
  --num_epochs 10 \
  --learning_rate 3e-5 \
  --batch_size 2 \
  --gradient_accumulation_steps 8 \
  --lora_r 32 \
  --lora_alpha 64
```

## ‚öñÔ∏è Legal Consultation Examples

### Single Question
```bash
python inference.py \
  --base_model neuralmind/bert-base-portuguese-cased \
  --peft_model ./legal_model \
  --question "Como renovar o t√≠tulo de resid√™ncia?" \
  --context "T√≠tulo expira em 3 meses"
```

### Interactive Mode
```bash
python inference.py \
  --base_model neuralmind/bert-base-portuguese-cased \
  --peft_model ./legal_model \
  --interactive
```

Example interaction:
```
üèõÔ∏è  Consulta Jur√≠dica - Imigra√ß√£o Portugal üáµüáπ
üíº Sua pergunta jur√≠dica: Posso trabalhar em Portugal com visto D7?
üìã Contexto adicional: Tenho visto D7 para aposentados

üìù Resposta Legal:
O visto D7 n√£o permite atividade profissional dependente...
```

## üîß Model Architecture

```
Input: [LEGAL_Q] {question} [IMMIGRATION] {context}
       ‚Üì
BERT Encoder (Portuguese) ‚Üí Learned Representations
       ‚Üì
BERT Decoder ‚Üí [LEGAL_A] {legal_advice}
```

### Special Legal Tokens
- `[LEGAL_Q]`: Marks legal questions
- `[LEGAL_A]`: Marks legal answers  
- `[IMMIGRATION]`: Immigration context
- `[PORTUGAL]`: Portugal-specific content
- `[VISA]`, `[RESIDENCY]`, `[CITIZENSHIP]`: Document types
- `[DOCUMENT]`: General document reference

## üìä Legal Domains Covered

1. **Work Visas & Permits**
   - Employment authorization
   - Work contract requirements
   - Professional qualifications

2. **Residency Permits**
   - Initial applications
   - Renewals and extensions
   - Change of status

3. **Family Reunification**
   - Spouse and children
   - Dependent relatives
   - Documentation requirements

4. **Citizenship & Naturalization**
   - Residency requirements
   - Language proficiency
   - Application process

5. **Student Visas**
   - University enrollment
   - Work permissions
   - Extension procedures

6. **Retirement Visas (D7)**
   - Income requirements
   - Investment options
   - Activity restrictions

## üíæ Hardware Requirements

| Component | Minimum | Recommended |
|-----------|---------|-------------|
| GPU Memory | 8GB (RTX 3060 Ti) | 16GB+ (RTX 4070 Ti) |
| System RAM | 16GB | 32GB+ |
| Storage | 20GB | 50GB+ SSD |

### Memory Optimization Tips
```bash
# For 8GB GPU
python main.py \
  --batch_size 1 \
  --gradient_accumulation_steps 16 \
  --max_length 256 \
  --lora_r 8 \
  --fp16

# For 16GB+ GPU  
python main.py \
  --batch_size 4 \
  --gradient_accumulation_steps 4 \
  --max_length 512 \
  --lora_r 16
```

## üõ†Ô∏è Advanced Usage

### Create Legal Dataset
```bash
# Generate comprehensive legal immigration dataset
python data_utils.py create-legal

# Validate your legal data
python data_utils.py validate legal_data.json

# Split into train/val/test
python data_utils.py split legal_data.json --train_ratio 0.8 --val_ratio 0.1
```

### Multi-GPU Training
```bash
accelerate config
accelerate launch main.py --data_path legal_data.json
```

### Custom Legal Tokens
Edit the model configuration to add domain-specific tokens:
```python
special_tokens = {
    "additional_special_tokens": [
        "[LEGAL_Q]", "[LEGAL_A]", "[IMMIGRATION]", 
        "[CUSTOM_DOMAIN]", "[SPECIFIC_LAW]"
    ]
}
```

## ‚ö†Ô∏è Important Legal Disclaimer

**This model provides informational responses only and does not constitute legal advice. Always consult with a qualified immigration lawyer for official legal guidance.**

## üìà Model Performance

Typical training results on Portuguese immigration law:

| Metric | Value |
|--------|-------|
| Training Loss | 0.8-1.2 |
| Evaluation Loss | 1.0-1.5 |
| Response Relevance | 85-92% |
| Legal Accuracy | 78-85% |

## üìÅ Project Structure

```
LLM-fine-tuning/
‚îú‚îÄ‚îÄ model.py                 # LegalBertSeq2Seq model class
‚îú‚îÄ‚îÄ trainer.py              # Legal PEFT training pipeline  
‚îú‚îÄ‚îÄ main.py                 # Training script for legal model
‚îú‚îÄ‚îÄ inference.py            # Legal consultation interface
‚îú‚îÄ‚îÄ data_utils.py          # Legal data processing utilities
‚îú‚îÄ‚îÄ config.json            # Legal model configuration
‚îú‚îÄ‚îÄ requirements.txt       # Dependencies
‚îî‚îÄ‚îÄ README.md             # This documentation
```

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch for legal improvements
3. Add legal domain expertise or new immigration topics
4. Submit a pull request with legal validation

## üìÑ License

This project is licensed under the MIT License. See [LICENSE](LICENSE) for details.

## üôè Acknowledgments

- **NeuralMind** for the Portuguese BERT model
- **HuggingFace** for Transformers and PEFT libraries  
- **Portuguese immigration law experts** for domain knowledge
- **SEF/AIMA** for official immigration procedures

## Features

- **PEFT Integration**: Uses LoRA (Low-Rank Adaptation) for memory-efficient fine-tuning
- **HuggingFace Integration**: Built on Transformers and PEFT libraries
- **Flexible Data Format**: Supports instruction-following datasets
- **Memory Optimization**: Gradient checkpointing and mixed precision training
- **Easy Configuration**: JSON config files and command-line arguments
- **Inference Tools**: Interactive chat and single instruction inference

## Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd LLM-fine-tuning
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. (Optional) Install with CUDA support:
```bash
# For CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

## Data Format

The training data should be in JSON format with the following structure:

```json
[
  {
    "instruction": "Your instruction here",
    "input": "Optional input context",
    "output": "Expected output/response"
  },
  {
    "instruction": "Another instruction",
    "input": "",
    "output": "Response without input context"
  }
]
```

## Quick Start

### 1. Create Sample Data and Train

```bash
# Create sample data and start training
python main.py --data_path sample_data.json --create_sample_data --max_steps 50
```

### 2. Custom Training

```bash
# Train with your own data
python main.py \
  --data_path your_data.json \
  --output_dir ./my_fine_tuned_model \
  --num_epochs 3 \
  --learning_rate 2e-4 \
  --batch_size 4 \
  --gradient_accumulation_steps 4 \
  --lora_r 16 \
  --lora_alpha 32
```

### 3. Advanced Training Options

```bash
python main.py \
  --model_name deepseek-ai/deepseek-r1 \
  --data_path training_data.json \
  --output_dir ./results \
  --max_length 1024 \
  --num_epochs 5 \
  --learning_rate 1e-4 \
  --batch_size 2 \
  --gradient_accumulation_steps 8 \
  --lora_r 32 \
  --lora_alpha 64 \
  --lora_dropout 0.05 \
  --warmup_steps 200 \
  --save_steps 250 \
  --logging_steps 5 \
  --fp16 \
  --gradient_checkpointing
```

## Inference

### Interactive Chat Mode

```bash
python inference.py \
  --base_model deepseek-ai/deepseek-r1 \
  --peft_model ./fine_tuned_model \
  --interactive
```

### Single Instruction

```bash
python inference.py \
  --base_model deepseek-ai/deepseek-r1 \
  --peft_model ./fine_tuned_model \
  --instruction "Explain machine learning" \
  --max_length 512
```

### With Input Context

```bash
python inference.py \
  --base_model deepseek-ai/deepseek-r1 \
  --peft_model ./fine_tuned_model \
  --instruction "Translate to Spanish" \
  --input "Hello, how are you?" \
  --temperature 0.3
```

## Configuration

### LoRA Parameters

- **r (rank)**: Controls the rank of adaptation matrices (4-64, default: 16)
- **alpha**: Scaling parameter (typically 2x rank, default: 32)
- **dropout**: Dropout for LoRA layers (0.0-0.3, default: 0.1)
- **target_modules**: Which layers to apply LoRA to

### Training Parameters

- **learning_rate**: Learning rate (1e-5 to 5e-4, default: 2e-4)
- **batch_size**: Per-device batch size (1-8, depending on GPU memory)
- **gradient_accumulation_steps**: Effective batch size multiplier
- **max_length**: Maximum sequence length (128-2048)

### Memory Optimization

- **fp16**: Mixed precision training (reduces memory by ~50%)
- **gradient_checkpointing**: Trade compute for memory
- **gradient_accumulation_steps**: Simulate larger batch sizes

## File Structure

```
LLM-fine-tuning/
‚îú‚îÄ‚îÄ main.py              # Main training script
‚îú‚îÄ‚îÄ trainer.py           # PEFT training pipeline
‚îú‚îÄ‚îÄ model.py             # Model wrapper classes
‚îú‚îÄ‚îÄ inference.py         # Inference and chat interface
‚îú‚îÄ‚îÄ config.json          # Configuration file
‚îú‚îÄ‚îÄ requirements.txt     # Python dependencies
‚îî‚îÄ‚îÄ README.md           # This file
```

## Hardware Requirements

### Minimum Requirements
- **GPU**: 8GB VRAM (RTX 3060 Ti, RTX 4060 Ti)
- **RAM**: 16GB system RAM
- **Storage**: 20GB free space

### Recommended Requirements
- **GPU**: 16GB+ VRAM (RTX 4070 Ti, RTX 4080, A4000)
- **RAM**: 32GB+ system RAM
- **Storage**: 50GB+ SSD space

### Memory Usage Tips

1. **Reduce batch size**: Start with `--batch_size 1`
2. **Increase gradient accumulation**: Use `--gradient_accumulation_steps 16`
3. **Enable optimizations**: Use `--fp16` and `--gradient_checkpointing`
4. **Reduce sequence length**: Use `--max_length 512` or lower
5. **Lower LoRA rank**: Use `--lora_r 8` for less memory usage

## Troubleshooting

### CUDA Out of Memory
```bash
# Reduce memory usage
python main.py \
  --batch_size 1 \
  --gradient_accumulation_steps 16 \
  --max_length 256 \
  --lora_r 8 \
  --fp16 \
  --gradient_checkpointing
```

### Slow Training
- Increase batch size if you have memory
- Use multiple GPUs with `accelerate`
- Reduce `--max_length` if sequences are long

### Poor Results
- Increase `--lora_r` and `--lora_alpha`
- Use more training data
- Increase `--num_epochs`
- Tune `--learning_rate` (try 1e-4 or 5e-5)

## Advanced Usage

### Multi-GPU Training

```bash
# Install accelerate
pip install accelerate

# Configure accelerate
accelerate config

# Run with accelerate
accelerate launch main.py --data_path your_data.json
```

### Custom LoRA Configuration

Edit `config.json` to customize LoRA parameters:

```json
{
  "lora_config": {
    "r": 32,
    "alpha": 64,
    "dropout": 0.05,
    "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"]
  }
}
```

### Monitoring Training

The trainer automatically logs to the console. For more detailed monitoring:

```bash
# View training logs
tail -f ./results/runs/*/events.out.tfevents.*

# Or use tensorboard
tensorboard --logdir ./results/runs
```

## Model Performance

Typical training times on different hardware:

| GPU | Batch Size | Time/Epoch | Memory Usage |
|-----|------------|------------|--------------|
| RTX 3060 Ti (8GB) | 1 | ~2 hours | 7.5GB |
| RTX 4070 Ti (12GB) | 2 | ~1.5 hours | 11GB |
| RTX 4080 (16GB) | 4 | ~1 hour | 14GB |
| A6000 (48GB) | 8 | ~30 min | 32GB |

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## Support

For issues and questions:
1. Check the troubleshooting section
2. Search existing GitHub issues
3. Create a new issue with detailed information

## Acknowledgments

- DeepSeek AI for the base model
- HuggingFace for the Transformers and PEFT libraries
- The open-source community for tools and inspiration
